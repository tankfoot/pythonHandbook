{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-09T22:16:12.534253Z",
     "start_time": "2018-01-09T22:16:12.530354Z"
    }
   },
   "source": [
    "# Homework 1: A Simple Sentiment Analysis of Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this homework is to assess your current skills and your ability to solve an unknown problem. To successfully complete this homework, you may use any resources available to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-09T22:43:29.993598Z",
     "start_time": "2018-01-09T22:43:29.974919Z"
    }
   },
   "source": [
    "You need to accomplish the following tasks:\n",
    "1. Choose a Twitter API.\n",
    "1. Configure access to the Twitter API\n",
    "2. Identify a trending topic on Twitter.\n",
    "3. Get at least 500 tweets on your trending topic.\n",
    "3. Find lists of stopwords, positive words, and negative words.\n",
    "4. Calculate the ratio of positive to negative words in your sample.\n",
    "    \n",
    "Answer the following questions: \n",
    "* __What is the ratio of positive to negative words on your trending topic?__ \n",
    "* __What is your interpretation of the ratio?__\n",
    "* __What is the managerial insight that you could offer based on your results?__\n",
    "\n",
    "If you use tutorials/code snippets that you find on the internet to complete this task, make sure that you reference them. Also make sure that the Jupyter notebook is free of mistakes, well-documented, and professionally formatted before you submit it.\n",
    "\n",
    "This homework is due on **Tuesday, 16 2018**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CAUTION:** This notebook is purely for instructional purposes and not intended not suitable for use in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T04:56:38.080860Z",
     "start_time": "2018-01-12T04:56:38.071587Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a package to access the Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T16:02:07.543782Z",
     "start_time": "2018-01-10T16:02:07.533807Z"
    }
   },
   "source": [
    "I use the [python-twitter](https://github.com/bear/python-twitter) package to access the Twitter API. There are quite a [few packages to access Twitter](https://developer.twitter.com/en/docs/developer-utilities/twitter-libraries). I chose python-twitter because it seems to be reasonable well maintained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's download the package. The documentation for python-twitter is [here](https://python-twitter.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T16:03:20.363197Z",
     "start_time": "2018-01-10T16:03:15.537606Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install python-twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T16:16:51.226050Z",
     "start_time": "2018-01-10T16:16:51.221333Z"
    }
   },
   "source": [
    "## Configure access to the Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T16:18:58.533377Z",
     "start_time": "2018-01-10T16:18:58.522214Z"
    }
   },
   "source": [
    "You will find a step-by-step how-to to access the Twitter API. You will need a Twitter account to create a Twitter app. For this app, you will need four impotant keys:\n",
    "* consumer_key\n",
    "* consumer_secret\n",
    "* access_token_key\n",
    "* access_token_secret\n",
    "\n",
    "These keys have to be **secret** and **must not** be shared on Github. Thus, we put them in a config file `config.cfg` that we read during initialization of the API. **Add the config file to your .gitignore file**, so that you do not accidentially upload it.\n",
    "\n",
    "The config file has the following structure:\n",
    "\n",
    "`[twitter]\n",
    "consumer_key=AAAAAAAA\n",
    "consumer_secret=BBBBBBBB\n",
    "access_token_key=CCCCCCC\n",
    "access_token_secret=DDDDDDD`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, let's test the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we need two packages:\n",
    "* The confiparser allows us to read a config file\n",
    "* The twitter allows us to access the twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T04:56:42.862365Z",
     "start_time": "2018-01-12T04:56:42.527672Z"
    }
   },
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "import twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the config.cfg file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T04:56:43.553168Z",
     "start_time": "2018-01-12T04:56:43.510845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.cfg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConfigParser()\n",
    "config.read('config.cfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup access to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T04:56:44.521111Z",
     "start_time": "2018-01-12T04:56:44.497648Z"
    }
   },
   "outputs": [],
   "source": [
    "api = twitter.Api(consumer_key=config.get('twitter','consumer_key'), \n",
    "                  consumer_secret=config.get('twitter', 'consumer_secret'), \n",
    "                  access_token_key=config.get('twitter', 'access_token_key'),\n",
    "                  access_token_secret=config.get('twitter', 'access_token_secret'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T20:18:35.848631Z",
     "start_time": "2018-01-10T20:18:35.842789Z"
    }
   },
   "source": [
    "## Identify a trending topic on Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T04:56:46.944012Z",
     "start_time": "2018-01-12T04:56:46.938009Z"
    }
   },
   "outputs": [],
   "source": [
    "topic = 'trump'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get at least 500 tweets on your trending topic (Part I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We search for the topic. The result is a list of `twitter.models.Status` objects. The search functionality is limited to [100 tweets](https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets). We come back to that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T04:56:49.046242Z",
     "start_time": "2018-01-12T04:56:48.465772Z"
    }
   },
   "outputs": [],
   "source": [
    "results = api.GetSearch(\n",
    "    raw_query=\"q=\"+topic+\"&result_type=recent&count=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T18:39:56.849451Z",
     "start_time": "2018-01-10T18:39:56.840567Z"
    }
   },
   "source": [
    "The object `twitter.models.Status` offers a text attribute that contains the actual tweet text. We get the texts by \"looping\" through the results in Python style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T04:57:48.333597Z",
     "start_time": "2018-01-12T04:57:48.327410Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = [result.text for result in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find lists of stopwords, positive words, and negative words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text analysis is a common problem in data science. Luckily, very smart and capable people have developed powerful packages that already do most of the work. For text analysis, a great package is [NLTK](http://www.nltk.org). Let's install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T19:11:59.606500Z",
     "start_time": "2018-01-10T19:11:58.736984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from nltk)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are able to import NLTK. The task is to count the positive and negative words and exclude stopwords. NLTK already provides a list of stopwords. We download the list and import it. Then, we tokenize the tweets, which means that we separate each word. (We need the `punkt` list from NLTK for this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T19:11:59.680341Z",
     "start_time": "2018-01-10T19:11:59.627137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T19:11:59.717407Z",
     "start_time": "2018-01-10T19:11:59.708915Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T18:46:38.167342Z",
     "start_time": "2018-01-10T18:46:38.160135Z"
    }
   },
   "source": [
    "We apply the function `word_tokenize` to each tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T19:11:59.792178Z",
     "start_time": "2018-01-10T19:11:59.743986Z"
    }
   },
   "outputs": [],
   "source": [
    "token_text = [word_tokenize(str(text)) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remove all the stopword that the list `sw` provides. Remember, we have a tokenized list now, which is essentially a list of lists. For each tweet, we retain all the words that are not stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T19:11:59.898575Z",
     "start_time": "2018-01-10T19:11:59.888031Z"
    }
   },
   "outputs": [],
   "source": [
    "relevant_words = []\n",
    "for t in token_text:\n",
    "    rw = [w for w in t if str(w).lower() not in sw]\n",
    "    relevant_words.append(rw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has the VADER Sentiment Analyzer, wich is particularly interesting for Social Media. Read more about VADER here:\n",
    "\n",
    ">Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n",
    "\n",
    "We import VADER, download the lexicon, and create an Analyzer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T19:12:00.254238Z",
     "start_time": "2018-01-10T19:12:00.221586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /home/dv/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal was to count the positive and negative words. Thus, we flatten the list of tweets. We also create a function to get the compound scores from Vader and map this to the list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T20:13:09.540023Z",
     "start_time": "2018-01-10T20:13:09.462418Z"
    }
   },
   "outputs": [],
   "source": [
    "flatten_relevant_words = [word for text in relevant_words for word in text]\n",
    "\n",
    "def get_compound_score(word):\n",
    "    if word is not None:\n",
    "        return sid.polarity_scores(word)['compound']\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "scores = list( map(get_compound_score, flatten_relevant_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the ratio of positive to negative words in your sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we simply count the positive and negative words. We can also count the neutral words. (Caution: we have a very broad definition of what counts as positive or negative.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T19:12:00.960242Z",
     "start_time": "2018-01-10T19:12:00.947712Z"
    }
   },
   "outputs": [],
   "source": [
    "pos = sum(1 for score in scores if score > 0)\n",
    "neg = sum(1 for score in scores if score < 0)\n",
    "neu = sum(1 for score in scores if score == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can calculate the ratio of postive to negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T19:12:01.560245Z",
     "start_time": "2018-01-10T19:12:01.552878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.727272727272727"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pos-neg)/neg)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T19:07:53.909984Z",
     "start_time": "2018-01-10T19:07:53.901049Z"
    }
   },
   "source": [
    "We can do the same procedure on the full tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T20:06:22.729083Z",
     "start_time": "2018-01-10T20:06:22.647815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.21212121212121"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tweet_scores = list( map(get_compound_score, texts))\n",
    "pos = sum(1 for score in full_tweet_scores if score > 0)\n",
    "neg = sum(1 for score in full_tweet_scores if score < 0)\n",
    "neu = sum(1 for score in full_tweet_scores if score == 0)\n",
    "((pos-neg)/neg)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get at least 500 tweets on your trending topic (Part II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, our sample is limited by the search query, which only allows 100 tweets. Thus, we will collect tweets on our topic using the stream functionality of Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup a StreamFilter object and let it track the topic. Each tweet in this object will be appended to the list `streamed_tweets`. Once that list is 500 tweets long, we stop. The API documentation is a bit thin, but [this blog post](https://iseverythingstilltheworst.com/blog/2016/05/28/capturing_twitter_streams/) helped a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T20:13:41.347830Z",
     "start_time": "2018-01-10T20:13:27.084183Z"
    }
   },
   "outputs": [],
   "source": [
    "stream = api.GetStreamFilter(track=[topic])\n",
    "streamed_tweets = []\n",
    "\n",
    "for tweet in stream:\n",
    "    streamed_tweets.append(twitter.Status.NewFromJsonDict(tweet).text)\n",
    "    if len(streamed_tweets) == 500:\n",
    "        stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the same function on the streamed tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T20:14:22.632304Z",
     "start_time": "2018-01-10T20:14:22.442914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.771929824561402"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.087719298245614"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_scores = list( map(get_compound_score, streamed_tweets))\n",
    "pos = sum(1 for score in stream_scores if score > 0.5)\n",
    "neg = sum(1 for score in stream_scores if score < -0.5)\n",
    "neu = sum(1 for score in stream_scores if score == 0)\n",
    "((pos-neg)/neg)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T20:16:59.882499Z",
     "start_time": "2018-01-10T20:16:59.860169Z"
    }
   },
   "source": [
    "This gives us the current ratio of positive to negative tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up (Optional for Docker Use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have installed a couple of packages that we want to include in the docker container. We create a list of installed packages that we can use to updated the requirements for the docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T20:24:11.664630Z",
     "start_time": "2018-01-10T20:24:09.605143Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
